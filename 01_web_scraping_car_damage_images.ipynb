{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraping Car Damage Images from a Website"
      ],
      "metadata": {
        "id": "y4heJj6joy2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Reminder:*** *Before scraping any website, make sure to check the website policies and follow the rules on how to scrape websites ethically.*"
      ],
      "metadata": {
        "id": "W8iqM4fao-a3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HETcHKN4JgMY",
        "outputId": "55f0724f-42bc-4c76-8832-027c24fec28a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting undetected_chromedriver\n",
            "  Downloading undetected-chromedriver-3.5.5.tar.gz (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (17.0.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting selenium-wire\n",
            "  Downloading selenium_wire-5.1.0-py3-none-any.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from undetected_chromedriver) (14.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: blinker>=1.4 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (1.9.0)\n",
            "Collecting brotli>=1.0.9 (from selenium-wire)\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting kaitaistruct>=0.7 (from selenium-wire)\n",
            "  Downloading kaitaistruct-0.10-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyasn1>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (0.6.1)\n",
            "Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (24.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (3.2.1)\n",
            "Requirement already satisfied: pysocks>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (1.7.1)\n",
            "Collecting wsproto>=0.14 (from selenium-wire)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: zstandard>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (0.23.0)\n",
            "Requirement already satisfied: h2>=4.0 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (4.2.0)\n",
            "Requirement already satisfied: hyperframe>=6.0 in /usr/local/lib/python3.11/dist-packages (from selenium-wire) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2>=4.0->selenium-wire) (4.1.0)\n",
            "Requirement already satisfied: cryptography<44,>=41.0.5 in /usr/local/lib/python3.11/dist-packages (from pyOpenSSL>=22.0.0->selenium-wire) (43.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.1.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->selenium-wire) (0.14.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44,>=41.0.5->pyOpenSSL>=22.0.0->selenium-wire) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL>=22.0.0->selenium-wire) (2.22)\n",
            "Downloading selenium-4.28.1-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium_wire-5.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kaitaistruct-0.10-py2.py3-none-any.whl (7.0 kB)\n",
            "Downloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.1-py3-none-any.whl (21 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: undetected_chromedriver\n",
            "  Building wheel for undetected_chromedriver (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for undetected_chromedriver: filename=undetected_chromedriver-3.5.5-py3-none-any.whl size=47048 sha256=9de88e829a5ba7457df12fcea3faba51813ada99a3023fd29122af4d85d22bf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/b9/03/4b6e38f019d6170e8c25df2e1e362d7bdf9ff4012df2dc85c0\n",
            "Successfully built undetected_chromedriver\n",
            "Installing collected packages: sortedcontainers, brotli, wsproto, outcome, kaitaistruct, trio, trio-websocket, selenium, undetected_chromedriver, selenium-wire\n",
            "Successfully installed brotli-1.1.0 kaitaistruct-0.10 outcome-1.3.0.post0 selenium-4.28.1 selenium-wire-5.1.0 sortedcontainers-2.4.0 trio-0.29.0 trio-websocket-0.12.1 undetected_chromedriver-3.5.5 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium undetected_chromedriver beautifulsoup4 pandas pyarrow Pillow requests selenium-wire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dwAkyginabG"
      },
      "source": [
        "### **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMnyUdukJlyZ"
      },
      "outputs": [],
      "source": [
        "# import undetected_chromedriver as uc\n",
        "import hashlib, io, requests, pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver import ChromeOptions\n",
        "# from seleniumwire import webdriver\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from pathlib import Path\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prmfLvvOnfUl"
      },
      "source": [
        "### **Mount Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5U7qw4jJoK3",
        "outputId": "0c7e692c-6ae3-4e63-b2df-0f4051616988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U-46HGFnj2A"
      },
      "source": [
        "### **Launch the WebDriver to open a target URL**\n",
        "Configure Chrome Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulI0QSBwJq5H"
      },
      "outputs": [],
      "source": [
        "options = ChromeOptions()\n",
        "options.add_argument(\"--headless=new\")\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36\")\n",
        "# driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "driver = webdriver.Chrome(options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extracting Page URLs**\n",
        "Each url/page contains the photos for every listed car. Each page will have around 12 photos for the specific car brand. These contains internal and external photos of the car. We only need the external photos so we will be only be extracting those later on.\n",
        "\n",
        "**Note**: When you encounter a connection error to the website, just rerun the chrome configuration options cell."
      ],
      "metadata": {
        "id": "fOQxEOuPUOKh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z__ECvcsuq2J"
      },
      "source": [
        "Images are from AutobidMaster Website. URLS should be extracted for front, rear, and side panels. The same process should be repeated for each panel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy_rSc9zk9No"
      },
      "outputs": [],
      "source": [
        "## empty list to store the url\n",
        "page_urls = []\n",
        "\n",
        "## the range indicates the number of pages in the website\n",
        "## let's try 30 pages for now\n",
        "for i in range(1, 30):\n",
        "  url = f\"https://www.autobidmaster.com/en/search/make-toyota,lexus,suzuki/doc-type-c,s/damage-front+end/?page={i}\"\n",
        "  driver.get(url)\n",
        "  time.sleep(random.randint(1,30))\n",
        "  elements = driver.find_elements(By.CLASS_NAME, \"_1dGf-ymm\")\n",
        "\n",
        "  for item in elements:\n",
        "    # print(img)\n",
        "    page_urls.append(item.get_attribute('href'))\n",
        "\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the process until you finish extracting all urls in all pages"
      ],
      "metadata": {
        "id": "pLSinIgWnuyt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JqVqBUUgTc5",
        "outputId": "3c018e6b-360a-4007-a224-bf3dbad85329"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "870"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "## check how many page urls were extracted\n",
        "len(page_urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving Extracted Page URLs"
      ],
      "metadata": {
        "id": "Jwt7BLq6x-MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## save the urls to a csv file just in case you need it later on\n",
        "df = pd.DataFrame(page_urls)\n",
        "df.to_csv('scraped_page_urls_front.csv')"
      ],
      "metadata": {
        "id": "52gnlux_jXrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjJpvFRw3xuq"
      },
      "outputs": [],
      "source": [
        "## run this code in case runtime suddenly stops and you lose the list\n",
        "df = pd.read_csv(\"scraped_page_urls_front.csv\")\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extracting Image URLs**"
      ],
      "metadata": {
        "id": "vOBvpETeTFaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we will be extracting the urls of the images per page. Extracting a large number of urls is time-consuming so you can do the extraction by batch (depends on your preference).\n",
        "\n",
        "**Note**: When you encounter a connection error to the website, just rerun the chrome configuration options cell."
      ],
      "metadata": {
        "id": "REP-ezmfUjaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## empty list to store the image urls\n",
        "image_urls = []\n",
        "\n",
        "## the range indicates the number of links we extracted in the prior step\n",
        "## since running this code is time-consuming, you can do it by batch until you finish all links\n",
        "for page in page_urls[1:50]:\n",
        "  url = f\"{page}\"\n",
        "  driver.get(url)\n",
        "  time.sleep(random.randint(1,30))\n",
        "  image_elements = driver.find_elements(By.CLASS_NAME, \"_XTkv-img\")\n",
        "\n",
        "  for img in image_elements:\n",
        "    # print(img)\n",
        "    image_urls.append(img.get_attribute('data-src'))\n",
        "\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "qk1IFwZMTFJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the process until you finish extracting all image urls."
      ],
      "metadata": {
        "id": "XaboBDS1n7sq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## check how many image urls were extracted\n",
        "len(image_urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSxs3ocml8J4",
        "outputId": "5e164c36-61d8-4a02-a36b-228022595af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "667"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving Extracted Image URLs"
      ],
      "metadata": {
        "id": "R-znILuXz5an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## save the urls to a csv file just in case you need it later on\n",
        "df_image_urls = pd.DataFrame(image_urls)\n",
        "df_image_urls.to_csv('scraped_image_urls_front.csv')"
      ],
      "metadata": {
        "id": "QuQUVWYdmB3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## run this code in case runtime suddenly stops and you lose the list\n",
        "# df_image_urls = pd.read_csv(\"scraped_image_urls_front.csv\")\n",
        "# df_image_urls.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "r75VmiROjjwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cleaning the Image URLs**"
      ],
      "metadata": {
        "id": "n-Wn-xJW0Nyr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLqfmuuJ320T",
        "outputId": "535b6423-7807-4d8f-acf9-6b188be0b67b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "667"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "## create a list for the url\n",
        "image_urls = df_image_urls[0].tolist()\n",
        "\n",
        "## check how many images we have\n",
        "len(image_urls)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## remove any URLs that contain the substring \"_ful.jpg\"\n",
        "## images with substring \"_ful.jpg\" does not belong to the same car in the list\n",
        "image_urls = [x for x in image_urls if \"_ful.jpg\" not in x]"
      ],
      "metadata": {
        "id": "PxqZWwU0gX-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## checking the list\n",
        "image_urls"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TjTlA9VmRpz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon checking, the 5th url in each car image set is the best image to train the model for front damages. Hence, we are only keeping the 5th item in each set."
      ],
      "metadata": {
        "id": "XNHsXDQep8J3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBchA3YojsCM"
      },
      "outputs": [],
      "source": [
        "## the target string serves as an indication of a new set of car images\n",
        "target_string = \"/build/spa/images/\"\n",
        "result_links = []\n",
        "temp_links = []  # Temporary storage\n",
        "\n",
        "for link in image_urls:\n",
        "  if target_string in link:\n",
        "    result_links.append(temp_links[4])  ## Keep the 5th link as this is the front damage in a car\n",
        "    result_links.append(link)\n",
        "    temp_links = [] # Reset temporary list\n",
        "  else:\n",
        "    temp_links.append(link)\n",
        "\n",
        "result_links.extend(temp_links)  # Add any remaining links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHJwA-nVjTL6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "## checking resulting list\n",
        "print(\"There are a total of\", len(result_links), \"items in the list\")\n",
        "\n",
        "result_links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65fsN4fVBiPN"
      },
      "outputs": [],
      "source": [
        "## remove the unnecessary image in the list\n",
        "temp_url = [x for x in result_links if \"/build/spa/images/\" not in x]\n",
        "\n",
        "## replace the thumbnail as a full image\n",
        "new_urls = [url.replace(\"_thb\", \"_ful\") for url in temp_url]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLj2jkHmgA0X",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "## checking the final list of front car damage\n",
        "print(\"There are\", len(new_urls), \"in the list\")\n",
        "\n",
        "new_urls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwhrVn3dt0h4"
      },
      "source": [
        "### **Save Images to Drive Folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQCB7x-6CEAG"
      },
      "outputs": [],
      "source": [
        "## set folder path\n",
        "data_path = \"/content/drive/My Drive/Colab Notebooks/Data/Scraped Data/car-damage-dataset/data2a/combined/front\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1Ip96uVn4t_"
      },
      "outputs": [],
      "source": [
        "for img in new_urls:\n",
        "  ## Store the content from the URL to a variable\n",
        "  image_content = requests.get(img).content\n",
        "\n",
        "  ## Create a byte object out of image_content and store it in the variable image_file\n",
        "  image_file = io.BytesIO(image_content)\n",
        "\n",
        "  ## Use Pillow to convert the Python object to an RGB image\n",
        "  image = Image.open(image_file).convert(\"RGB\")\n",
        "\n",
        "  ## Set a file_path variable that points to your directory.\n",
        "  ## Create a file based on the sha1 hash of 'image_content'.\n",
        "  ## Use .hexdigest to convert it into a string.\n",
        "  file_path = Path(data_path, hashlib.sha1(image_content).hexdigest()[:10] + \".jpeg\")\n",
        "  image.save(file_path, \"JPEG\", quality=120)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the same process for every car panel."
      ],
      "metadata": {
        "id": "SAOKC35lnmgL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PIE2jmcDnrPf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}